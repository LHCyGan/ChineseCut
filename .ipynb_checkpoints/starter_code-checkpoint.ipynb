{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建一个分词工具\n",
    "\n",
    "### Part 1  基于枚举方法来搭建中文分词工具\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率 word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式\n",
    "- [我们，学习，人工智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工智能，是，未来]\n",
    "- [我们，学习，人工，智能，人工，智能，是，未来]\n",
    "- [我们，学习，人工智能，人工，智能，是，未来]\n",
    ".......\n",
    "\n",
    "\n",
    "#### Step 2: 我们也可以计算出每一个切分之后句子的概率\n",
    "- p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工，智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工)-log p(智能)-log p(是)-log p(未来)\n",
    "- p(我们，学习，人工智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工智能)-log p(人工)-log p(智能)-log(是)-log p(未来)\n",
    ".....\n",
    "\n",
    "#### Step 3: 返回第二步中概率最大的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'work_shet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c802ccf38c2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdic\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdic_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"综合类中文词库.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# 保存词典库中读取的单词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 以下是每一个单词出现的概率。为了问题的简化，我们只列出了一小部分单词的概率。 在这里没有出现的的单词但是出现在词典里的，统一把概率设置成为0.00001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c802ccf38c2c>\u001b[0m in \u001b[0;36mdataRead\u001b[1;34m(xpath, sheet_index)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_sheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwork_shet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtemp_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemp_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'work_shet' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: 第一步： 从综合类中文词库.xlsx 中读取所有中文词。\n",
    "#  hint: 思考一下用什么数据结构来存储这个词典会比较好？ 要考虑我们每次查询一个单词的效率。 \n",
    "def dataRead(xpath, sheet_index=0):\n",
    "    work_book = xlrd.open_workbook(filename=xpath)\n",
    "    work_sheet = work_book.sheet_by_index(sheet_index)\n",
    "    dic_words = {}\n",
    "    max_length = 0\n",
    "    for idx in range(work_sheet.nrows):\n",
    "        word = work_sheet.row(idx)[0].value.strip()\n",
    "        temp_length = len(word)\n",
    "        if temp_length > max_length:\n",
    "            max_length = temp_length\n",
    "        dic_words[word] = 0.00001\n",
    "    return dic-words, max_length\n",
    "\n",
    "dic_words, max_length = dataRead(\"综合类中文词库.xlsx\")    # 保存词典库中读取的单词\n",
    "\n",
    "# 以下是每一个单词出现的概率。为了问题的简化，我们只列出了一小部分单词的概率。 在这里没有出现的的单词但是出现在词典里的，统一把概率设置成为0.00001\n",
    "# 比如 p(\"学院\")=p(\"概率\")=...0.00001\n",
    "\n",
    "word_prob = {\"北京\":0.03,\"的\":0.08,\"天\":0.005,\"气\":0.005,\"天气\":0.06,\"真\":0.04,\"好\":0.05,\"真好\":0.04,\"啊\":0.01,\"真好啊\":0.02, \n",
    "             \"今\":0.01,\"今天\":0.07,\"课程\":0.06,\"内容\":0.06,\"有\":0.05,\"很\":0.03,\"很有\":0.04,\"意思\":0.06,\"有意思\":0.005,\"课\":0.01,\n",
    "             \"程\":0.005,\"经常\":0.08,\"意见\":0.08,\"意\":0.01,\"见\":0.005,\"有意见\":0.02,\"分歧\":0.04,\"分\":0.02, \"歧\":0.005}\n",
    "\n",
    "for key, value in word_prob.items():\n",
    "    dic_words[key] = value\n",
    "print (sum(word_prob.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-04c3acee1582>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-04c3acee1582>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    best_segment =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def word_cut_native(input_str):\n",
    "    # 递归的方式枚举出所有的可能\n",
    "    length_str= len(input_str)\n",
    "    \n",
    "    segments = []\n",
    "    if length_str == 0:\n",
    "        return segments\n",
    "    \n",
    "    max_split = min(max_length, length_str) + 1\n",
    "    for ids in range(1, max_split):\n",
    "        word = input_str[0:ids]\n",
    "        \n",
    "        if word in dic_words:\n",
    "            segment_words = word_cut_native(input_str[ids:])\n",
    "            if len(segment_words) == 0:\n",
    "                segments.append([word])\n",
    "            else:\n",
    "                for element in segment_words:\n",
    "                    element =[word]+element\n",
    "                    segments.append(element)\n",
    "    return segments\n",
    "    \n",
    "\n",
    "\n",
    "## TODO 请编写word_segment_naive函数来实现对输入字符串的分词\n",
    "def word_segment_naive(input_str):\n",
    "    \"\"\"\n",
    "    1. 对于输入字符串做分词，并返回所有可行的分词之后的结果。\n",
    "    2. 针对于每一个返回结果，计算句子的概率\n",
    "    3. 返回概率最高的最作为最后结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "\n",
    "    length_input_str = len(input_str)\n",
    "    # TODO： 第一步： 计算所有可能的分词结果，要保证每个分完的词存在于词典里，这个结果有可能会非常多。 \n",
    "    segments = word_cut_native(input_str) # 存储所有分词的结果。如果次字符串不可能被完全切分，则返回空列表(list)\n",
    "                   # 格式为：segments = [[\"今天\"，“天气”，“好”],[\"今天\"，“天“，”气”，“好”],[\"今“，”天\"，“天气”，“好”],...]\n",
    "    \n",
    "    # TODO: 第二步：循环所有的分词结果，并计算出概率最高的分词结果，并返回\n",
    "    best_segment = []\n",
    "    best_score = np.inf\n",
    "    for seg in segments:\n",
    "        # TODO ...\n",
    "        log_sum= -1 * np.sum(np.log([dic_words[word]+EPSILON  for word in seg]))\n",
    "        if log_sum <best_score:\n",
    "            best_score=log_sum\n",
    "            best_segment=seg\n",
    "    \n",
    "    return best_segment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-830f0e9adef8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-830f0e9adef8>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print (word_segment_naive(\"经常有意见分歧\"))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_naive(\"北京的天气真好啊\"))\n",
    "print (word_segment_naive(\"今天的课程内容很有意思\")\n",
    "print (word_segment_naive(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2  基于维特比算法来优化上述流程\n",
    "\n",
    "此项目需要的数据：\n",
    "1. 综合类中文词库.xlsx： 包含了中文词，当做词典来用\n",
    "2. 以变量的方式提供了部分unigram概率word_prob\n",
    "\n",
    "\n",
    "举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15\n",
    "\n",
    "#### Step 1: 根据词典，输入的句子和 word_prob来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
    "有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率已经给出（存放在word_prob）。\n",
    "注意：思考用什么方式来存储这种有向图比较合适？ 不一定只有一种方式来存储这种结构。 \n",
    "\n",
    "#### Step 2: 编写维特比算法（viterebi）算法来找出其中最好的PATH， 也就是最好的句子切分\n",
    "具体算法参考课程中讲过的内容\n",
    "\n",
    "#### Step 3: 返回结果\n",
    "跟PART 1的要求一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-29f9ecd65d37>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-29f9ecd65d37>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    graph =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def build_graph(input_str):\n",
    "    # 构件图\n",
    "    chars = list(input_str)\n",
    "    chars_length = len(chars)\n",
    "    word_list = [[0 for _ in range(chars_length)] for _ in range(chars_length)]\n",
    "    \n",
    "    for i in range(chars_length):\n",
    "        for j in range(i, chars_length):\n",
    "            if input_str[i:j+1] in dic_words:\n",
    "                key = input_str[i:j+1]\n",
    "                word_list[i][j] = dic_words[key]\n",
    "                \n",
    "    graph = pd.DataFrame(data=np.array(word_list), index=list(range(chars_length)))\n",
    "    graph = -1 * np.log(graph + EPSILON)\n",
    "    return graph\n",
    "\n",
    "## TODO 请编写word_segment_viterbi函数来实现对输入字符串的分词\n",
    "def word_segment_viterbi(input_str):\n",
    "    \"\"\"\n",
    "    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。\n",
    "    2. 编写维特比算法来寻找最优的PATH\n",
    "    3. 返回分词结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: 第一步：根据词典，输入的句子，以及给定的unigram概率来创建带权重的有向图（Directed Graph） 参考：课程内容\n",
    "    #      有向图的每一条边是一个单词的概率（只要存在于词典里的都可以作为一个合法的单词），这些概率在 word_prob，如果不在word_prob里的单词但在\n",
    "    #      词典里存在的，统一用概率值0.00001。\n",
    "    #      注意：思考用什么方式来存储这种有向图比较合适？ 不一定有只有一种方式来存储这种结构。 \n",
    "    graph = build_graph(input_str)\n",
    "    num_chars = len(input_str)\n",
    "    distances_min = pd.Series(data=np.zeros(num_chars  + 1), index=list(range(num_char)))\n",
    "    path =  path=pd.Series( data=np.zeros(shape=(num_chars + 1,),dtype=np.int),index=list(range(num_chars + 1)))\n",
    "    # TODO： 第二步： 利用维特比算法来找出最好的PATH， 这个PATH是P(sentence)最大或者 -log P(sentence)最小的PATH。\n",
    "    #              hint: 思考为什么不用相乘: p(w1)p(w2)...而是使用negative log sum:  -log(w1)-log(w2)-...\n",
    "    for idx_end in range(1, num_chars+1):\n",
    "        possible_path_distances = np.zeros((idx_end, ))\n",
    "        for idx_start in range(idx_end):\n",
    "            possible_path_distances[idx_start] = distances_min[idx_start] + graph.loc[idx_start, idx_end]\n",
    "        distances_min[idx_end] = np.min(possible_path_distances)\n",
    "    # TODO: 第三步： 根据最好的PATH, 返回最好的切分\n",
    "    best_segment = []\n",
    "    idx = num_chars\n",
    "    while idx > 0:\n",
    "        best_segment.append(input_str[path.loc[idx]:idx])\n",
    "        idx = path.loc[idx]\n",
    "    best_segment.reverse()\n",
    "    return best_segment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-830f0e9adef8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-830f0e9adef8>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print (word_segment_naive(\"经常有意见分歧\"))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "print (word_segment_naive(\"北京的天气真好啊\"))\n",
    "print (word_segment_naive(\"今天的课程内容很有意思\")\n",
    "print (word_segment_naive(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-6-2a298e6f8f52>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-2a298e6f8f52>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    第一个方法：\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: 第一种方法和第二种方法的时间复杂度和空间复杂度分别是多少？\n",
    "第一个方法： \n",
    "时间复杂度= , 空间复杂度=\n",
    "\n",
    "第二个方法：\n",
    "时间复杂度= , 空间复杂度="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-0c7c1ff5f492>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0c7c1ff5f492>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    - 0. （例）， 目前的概率是不完整的，可以考虑大量的语料库，然后从中计算出每一个词出现的概率，这样更加真实\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# TODO：如果把上述的分词工具持续优化，有哪些可以考虑的方法？ （至少列出3点）\n",
    "- 0. （例）， 目前的概率是不完整的，可以考虑大量的语料库，然后从中计算出每一个词出现的概率，这样更加真实\n",
    "- 1.\n",
    "- 2.\n",
    "- 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
